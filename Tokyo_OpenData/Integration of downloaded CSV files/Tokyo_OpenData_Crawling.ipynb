{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import requests\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.parse as par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : 도쿄 치안 데이터 : 치안 관련 키워드로 일본 도쿄 공공 데이터 수집\n",
    "#* 치안('治安'), 범죄('犯罪'), 경찰('警察'), 범죄 예방('犯罪予防'), CCTV('監視'), 범죄자('犯罪者'), 안전('安全'), 교통 안전('交通安全'), 경비원('警備員'), 범죄 통계('犯罪統計')\n",
    "#* 조회해보니 치안과 관련된 데이터 중 유용한 데이터는 '범죄('犯罪')' 키워드 뿐, 다른 데이터는 치안과 무관하거나 '범죄'키워드와 중복된다. \n",
    "#* 범죄('犯罪') 관련 키워드만 크롤링\n",
    "keyword = '犯罪'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded_text(keyword):\n",
    "    encoded = par.quote(keyword)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_page_info(keyword):\n",
    "    #* 첫 페이지 요청\n",
    "    encoded_keyword = encoded_text(keyword)\n",
    "    url = f'https://catalog.data.metro.tokyo.lg.jp/dataset?q={encoded_keyword}&sort=score+desc%2C+metadata_modified+desc'\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    #* 해당 키워드 전체 페이지 수\n",
    "    totalPageNum = len(soup.select('ul.pagination > li')) - 1\n",
    "    #print(f\"{keyword}의 전체 페이지 수:\", totalPageNum)\n",
    "    return totalPageNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_connect_url(keyword):\n",
    "    wholeurl = []\n",
    "    for idx in range(1, get_page_info(keyword) + 1):\n",
    "        encoded_keyword = encoded_text(keyword)\n",
    "        url = f'https://catalog.data.metro.tokyo.lg.jp/dataset?q={encoded_keyword}&sort=score+desc%2C+metadata_modified+desc&page={idx}'\n",
    "        wholeurl.append(url)\n",
    "    return wholeurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datanumList = []\n",
    "def get_connect_url(keyword):\n",
    "    for url in total_connect_url(keyword):\n",
    "        code = requests.get(url)\n",
    "        soup = BeautifulSoup(code.text, 'html.parser')\n",
    "        datanum = soup.select('ul.dataset-list.list-unstyled h3.dataset-heading > a')\n",
    "        connectBaseurl = 'https://catalog.data.metro.tokyo.lg.jp'\n",
    "        for idx in range(len(datanum)):\n",
    "            datanumList.append(connectBaseurl + datanum[idx].attrs['href'])\n",
    "    #print(len(datanumList))\n",
    "    return datanumList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chardet\n",
    "\n",
    "# with open('tokyo/fuc22a12pol01police.csv', 'rb') as rawdata:\n",
    "#     result = chardet.detect(rawdata.read(10000))\n",
    "\n",
    "# # check what the character encoding might be\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests.adapters\n",
    "\n",
    "#* 어댑터 사용 설정\n",
    "adapter = requests.adapters.HTTPAdapter()\n",
    "session = requests.Session()\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "get_csv_url = []\n",
    "\n",
    "try:\n",
    "    url = get_connect_url(keyword)\n",
    "    for idx in tqdm_notebook(range(len(url))):\n",
    "        response = session.get(url[idx])\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        tmp = soup.select('ul.resource-list > li > a')\n",
    "        baseurl = 'https://catalog.data.metro.tokyo.lg.jp'\n",
    "        for link in tmp:\n",
    "            get_csv_url.append(baseurl + link.get('href'))\n",
    "        \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"오류 발생:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm_notebook\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "#* urllib3 라이브러리에서 발생하는 InsecureRequestWarning 경고 메시지를 비활성화하는 역할\n",
    "#* 일반적으로 웹 요청 시 SSL 인증서의 유효성을 검사하고 경고 메시지를 표시하는데, \n",
    "#* InsecureRequestWarning은 SSL 인증서의 유효성 검사에 실패하여 보안상의 위험이 발생할 수 있음을 경고하는 메시지\n",
    "#* disable_warnings() 함수는 urllib3 라이브러리에서 경고 메시지를 비활성화하는 데 사용\n",
    "#* 이를 사용하여 InsecureRequestWarning 경고 메시지를 무시하고 프로그램을 실행가능\n",
    "#* 하지만 주의해야 할 점은 이렇게 경고 메시지를 비활성화하면 보안 위험을 감수해야 함\n",
    "#*  verify=False를 사용하여 인증서 검증을 비활성화\n",
    "\n",
    "final_csv_url = []\n",
    "\n",
    "for idx in tqdm_notebook(range(len(get_csv_url))):\n",
    "    url = get_csv_url[idx]\n",
    "    try:\n",
    "        code = requests.get(url, timeout=10, verify=False)\n",
    "        soup = BeautifulSoup(code.text, 'html.parser')\n",
    "        urlcsv = soup.select_one('p.text-muted.ellipsis > a').attrs['href']\n",
    "        final_csv_url.append(urlcsv)\n",
    "    except Exception as e:\n",
    "        print(f'||{urlcsv}|| 링크가 존재하지 않습니다.')\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "download_directory = 'tokyo'\n",
    "\n",
    "if not os.path.exists(download_directory):\n",
    "    os.makedirs(download_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import socket\n",
    "from urllib.error import URLError\n",
    "import pandas as pd\n",
    "\n",
    "#* ssl은 SSL 인증서 관련 작업을 위함\n",
    "#* socket은 연결 시간 초과와 관련된 예외 처리 위함\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "#*  SSL 인증서 검증을 비활성화, 이는 SSL 인증서가 검증되지 않아도 요청을 계속할 수 있게함\n",
    "for url in tqdm_notebook(final_csv_url):\n",
    "    try:\n",
    "        if url.endswith('.csv'):\n",
    "            file_name = url.split(\"/\")[-1]\n",
    "            pd.read_csv(url, encoding='SHIFT_JIS').to_csv('tokyo/' + file_name, encoding='SHIFT_JIS')\n",
    "    except URLError as e: #* URLError 예외가 발생한 경우, 연결 시간 초과(socket.timeout)인지 확인\n",
    "        if isinstance(e.reason, socket.timeout): #* isinstance(e.reason, socket.timeout)를 사용하여 예외의 reason이 socket.timeout인지 확인\n",
    "            print(f\"{file_name} 다운로드 중 연결 시간 초과가 발생했습니다.\")\n",
    "        else:\n",
    "            print(f\"{file_name} 다운로드 및 저장 중 오류가 발생했습니다: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{file_name} 다운로드 및 저장 중 오류가 발생했습니다: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
